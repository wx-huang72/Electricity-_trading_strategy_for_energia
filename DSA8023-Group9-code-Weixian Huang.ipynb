{
  "metadata": {
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Analytathon 3 ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Executive summary\nThe purpose of our project was to develop a model to predict prices by analysing market trends, which could help Energia purchase electricity at the best price among the 5 markets. \n\n\nTo make predictions, the model was trained on a dataset of historical sequence data. During training, the model learns to recognize patterns in the data and use them to make predictions about future values in the sequences. Once the model has been trained, it can be used to make predictions on new and unseen sequences, to help identify the optimal market to purchase electricity at 30-minute intervals for D+1.\n\n\nHowever, because of the complexity of the task and time constraints, we only managed to build a base model, that could give predictions but still could be improved. Therefore, further studies need to be carried out in order to explore how the model can be optimized to obtain a better prediction. \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Introduction\nEnergia Group is an Irish energy company that provides electricity and gas to customers in NI and ROI. Energia decided to buy electricity across 5 markets, and they decided to buy the electricity at the best price to optimise the business. The purpose of this report is to develop a Long Short Term Memory (LSTM) model to predict prices by analysing market trends, to guide Energia buying electricity at the lowest price. This report will also show our prediction of our model on the last day to demonstrate the performance of our model. \n\nThe report has been organised in the following way. We begins by focusing on organising the project into manageable tasks, followed by our metrics of success.It will then go on discussing data exploration in more detail. The third part is concerned with the methodology used for this study, we will explore the construction of LSTM model and the application of LSTM model on market price forecasting. Finally we will give our recommendations and discuss the potential limitations of our work. \n\nDue to time constraints and the complication of the project,  this project could not provide a perfect model to guide the purchase. We have only managed to build a model that can make predictions based on historical time series of the data. \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Data Processing\n\nFor this project, we have four variables that will affect the market price, which are demand, wind, gas price and electricity price. In the “prices_niv” file, the prices for all five markets of each 30-minute interval of everyday and national imbalance volume(NIV) are saved. We imported these five datasets into a jupyter notebook. Since some of the datasets are started from the second row and column, we formatted all the datasets immediately after importing by removing blank rows and blank columns. \n\nWe filter out the missing rows in “demand” dataset, insert the consequent time and date according its position, and set 0 to the demand variables, so now each data date and time period is continuous and uninterrupted.\n\nAfter inserting the missing rows,  we join all the datasets and filter the result dataframe by removing data whose date is after March 31, 2023. This is because the “prices_niv” stops at this data, to make sure we have all four variables for each slot to train the model, we choose to stop at this date. We convert all the data types to numeric as well in this step. The dataframe that merged all the data and contained all columns is called “combined”. ",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Load libraries",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": "# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# print all output from cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\";",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Import datasets",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# load datasets\ngas_prices = pd.read_excel(\"C:/Users/tomed/Downloads/Historic Gas Prices.xlsx\")\nwind = pd.read_excel(\"C:/Users/tomed/Downloads/Wind and Demand.xlsx\",  sheet_name='WIND')\ndemand = pd.read_excel(\"C:/Users/tomed/Downloads/Wind and Demand.xlsx\",  sheet_name='DEMAND')\nelectricity_prices = pd.read_excel(\"C:/Users/tomed/Downloads/GB Prices.xlsx\")\nprices_niv = pd.read_excel(\"C:/Users/tomed/Downloads/Prices and NIV.xlsx\")\n",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T15:33:33.840787Z",
          "iopub.status.busy": "2023-05-03T15:33:33.840467Z",
          "iopub.status.idle": "2023-05-03T15:33:49.448684Z",
          "shell.execute_reply": "2023-05-03T15:33:49.447688Z",
          "shell.execute_reply.started": "2023-05-03T15:33:33.840759Z"
        }
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Fix data formatting issues",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# prices_niv formatting\n# Correct columns and drop NaN column at index position [0] upon import\n\nprices_niv.columns = prices_niv.iloc[0]\nprices_niv = prices_niv.drop(prices_niv.index[0])\nprices_niv.drop(prices_niv.columns[[0]], axis = 1, inplace=True)\n# prices_niv.head()\n\n# wind formatting\n# Correct columns and drop NaN column at index position [0] upon import\n\nwind.columns = wind.iloc[0]\nwind = wind.drop(wind.index[0])\nwind.drop(wind.columns[[0]], axis = 1, inplace=True)\n# wind.head()\n\n# demand formatting\n# Correct columns and drop NaN column at index position [0] upon import\n\ndemand.columns = demand.iloc[0]\ndemand = demand.drop(demand.index[0])\ndemand.drop(demand.columns[[0]], axis = 1, inplace=True)\n# demand.head()",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T15:33:49.450751Z",
          "iopub.status.busy": "2023-05-03T15:33:49.450057Z",
          "iopub.status.idle": "2023-05-03T15:33:49.506460Z",
          "shell.execute_reply": "2023-05-03T15:33:49.505637Z",
          "shell.execute_reply.started": "2023-05-03T15:33:49.450712Z"
        }
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "demand.iloc[21601:21610]\ndemand.shape",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# 1. insert missing rows into demand\n\n#insert row in between index position 21604 and 21605\ndemand.loc[21604.5] = '27/03/2022', '27/03/2022', \"01:00:00\", 0\n\n#sort index\ndemand = demand.sort_index().reset_index(drop=True)\n\n# 2. insert missing rows into demand\n\n#insert row in between index position 21604 and 21605\ndemand.loc[21604.5] = '27/03/2022', '27/03/2022', \"01:30:00\", 0\n\n#sort index\ndemand = demand.sort_index().reset_index(drop=True)\n",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T15:33:49.541798Z",
          "iopub.status.busy": "2023-05-03T15:33:49.541493Z",
          "iopub.status.idle": "2023-05-03T15:33:49.577440Z",
          "shell.execute_reply": "2023-05-03T15:33:49.576506Z",
          "shell.execute_reply.started": "2023-05-03T15:33:49.541770Z"
        }
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "demand.iloc[21601:21610]\ndemand.shape",
      "metadata": {
        "tags": []
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# check shapes\nprices_niv.shape\nwind.shape\ndemand.shape",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# check null values\ndemand.isnull().sum()\nwind.isnull().sum()",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Combine dataframes",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "##### Join demand and wind",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# use demand to join others\n\n# Join wind\ndemand_wind = pd.merge(demand, wind, how = \"left\",on = [\"Trade Date\", \"Start Date\", \"Start Time 30 Minute Period\"])\ndemand_wind.tail(1)\ndemand_wind.isnull().sum()\n\ndemand_wind[\"Start Date\"] = pd.to_datetime(demand_wind[\"Start Date\"], format=\"%d/%m/%Y\")\n# Drop dates after 31/3/2023\ndemand_wind_1 = demand_wind\ndemand_wind = demand_wind_1[demand_wind_1['Start Date'] <= \"2023-03-31\"]\ndemand_wind.tail(1)\n\ndemand_wind.shape\ndemand_wind.isnull().sum()\n",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Join demand and electricity_prices",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Join electricity_prices\n\n# Rename heading to enable easy merge\nelectricity_prices.rename({'Start Time': 'Start Time 30 Minute Period'}, axis=1, inplace=True)\n# Merge using demand which has the full date range \ndemand_electricity = pd.merge(demand, electricity_prices, how = \"left\", on = [\"Trade Date\", \"Start Time 30 Minute Period\"])\n# Drop extra titles\ndemand_electricity.drop([\"Start Date Time\", \"Demand (MW)\"], axis=1, inplace = True)\ndemand_electricity.tail(1)\ndemand_electricity.isnull().sum()\n\ndemand_electricity[\"Start Date\"] = pd.to_datetime(demand_electricity[\"Start Date\"], format=\"%d/%m/%Y\")\n# Drop dates after 31/3/2023\ndemand_electricity_1 = demand_electricity\ndemand_electricity = demand_electricity_1[demand_electricity_1['Start Date'] <= \"2023-03-31\"]\ndemand_electricity.tail(1)\n\ndemand_electricity.shape\ndemand_electricity.isnull().sum()",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Join demand and gas",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Join gas\n\n# Get a column of the correct dates\nTradeDates = demand[[\"Trade Date\"]].copy()\nTradeDates.drop_duplicates(inplace = True)\nTradeDates.reset_index(drop=True, inplace = True)\n\n# Create column to join on and drop existing date\ngas_prices[\"Trade Date\"] = TradeDates\n# Merge\ndemand_gas_half_hour= pd.merge(gas_prices, demand, how='right', left_on='Trade Date', right_on='Trade Date')\ndemand_gas_half_hour.drop([\"Date\", \"Demand (MW)\"], axis=1, inplace = True)\ndemand_gas_half_hour.tail(1)\ndemand_gas_half_hour.isnull().sum()\n\n\ndemand_gas_half_hour[\"Start Date\"] = pd.to_datetime(demand_gas_half_hour[\"Start Date\"], format=\"%d/%m/%Y\")\n# Drop dates after 31/3/2023\ndemand_gas_half_hour_1 = demand_gas_half_hour\ndemand_gas_half_hour = demand_gas_half_hour_1[demand_gas_half_hour_1['Start Date'] <= \"2023-03-31\"]\ndemand_gas_half_hour.tail(1)\n\ndemand_gas_half_hour.shape\ndemand_gas_half_hour.isnull().sum()\n",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# set empty rows of IDA1 and IDA2 markets to 0 so that we can later join with demand dataframe and interpolate only the 8 NaN values we get\nprices_niv.isnull().sum()\n\n# replce NaNs with 0\nprices_niv.fillna(0, inplace=True)\n\n# validate\nprices_niv.isnull().sum()",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Join demand and price_niv",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Join price_niv\ndemand_price_niv = pd.merge(demand, prices_niv, how = \"left\",on = [\"Trade Date\", \"Start Date\", \"Start Time 30 Minute Period\"])\ndemand_price_niv.drop([\"Demand (MW)\"], axis=1, inplace = True)\ndemand_price_niv.tail(1)\ndemand_price_niv.isnull().sum()\n\n\ndemand_price_niv[\"Start Date\"] = pd.to_datetime(demand_price_niv[\"Start Date\"], format=\"%d/%m/%Y\")\n# Drop dates after 31/3/2023\ndemand_price_niv_1 = demand_price_niv\ndemand_price_niv = demand_price_niv_1[demand_price_niv_1['Start Date'] <= \"2023-03-31\"]\ndemand_price_niv.tail(1)\n\ndemand_price_niv.shape\ndemand_price_niv.isnull().sum()\n",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#check shapes after merge\ndemand_wind.shape\ndemand_electricity.shape\ndemand_gas_half_hour.shape\ndemand_price_niv.shape\n",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Final 'combined' dataframe",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Combine all the dataframes\ncombined = pd.merge(demand_price_niv, demand_gas_half_hour, on=[\"Start Date\", \"Trade Date\", \"Start Time 30 Minute Period\"]\n                    ).merge(demand_electricity, on=[\"Start Date\", \"Trade Date\", \"Start Time 30 Minute Period\"]\n                    ).merge(demand_wind, on=[\"Start Date\", \"Trade Date\", \"Start Time 30 Minute Period\"])\n\ncombined[\"Trade Date\"] = pd.to_datetime(combined[\"Trade Date\"], format=\"%d/%m/%Y\")\ncombined.shape\ncombined.isnull().sum()",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Interpolation",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We now have variables for each day and every time slot. Yet, there are still some rows that include 0 or NAN. These are the missing values that will impact the results and we need to replace them with a reasonable value. Since the interpolation could only be applied to 0 but not NAN, NAN was converted to 0 first. Apply `pd.interpolate()` to each variable column of “combined” and set the parameter `method = ‘pad’`. This parameter will copy the value just before the missing entry, this is the most sensible method among all the methods. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# check combined dataframe column data types\ncombined.info()",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# convert dtypes\ncombined['DAM Market Price (€/MWh)'] = pd.to_numeric(combined['DAM Market Price (€/MWh)'])\ncombined['IDA1 Market Price (€/MWh)'] = pd.to_numeric(combined['IDA1 Market Price (€/MWh)'])\ncombined['IDA2 Market Price (€/MWh)'] = pd.to_numeric(combined['IDA2 Market Price (€/MWh)'])\ncombined['IDA3 Market Price (€/MWh)'] = pd.to_numeric(combined['IDA3 Market Price (€/MWh)'])\ncombined['BM Market Price (€/MWh)'] = pd.to_numeric(combined['BM Market Price (€/MWh)'])\ncombined['Market Net Imbalance Volume (MW)'] = pd.to_numeric(combined['Market Net Imbalance Volume (MW)'])\ncombined['Demand (MW)'] = pd.to_numeric(combined['Demand (MW)'])\ncombined['Actual Wind (MW)'] = pd.to_numeric(combined['Actual Wind (MW)'])\n\n# check again\ncombined.info()",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# check for missing values\ncombined.isna().sum()\ncombined.isnull().sum()",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# perform interpolation for missing values\n# method='pad' will copy value just before the missing entry - made more sense when comparing other methods\n\ncombined['DAM Market Price (€/MWh)'] = combined['DAM Market Price (€/MWh)'].interpolate(method='pad')\ncombined['IDA1 Market Price (€/MWh)'] = combined['IDA1 Market Price (€/MWh)'].interpolate(method='pad')\ncombined['BM Market Price (€/MWh)'] = combined['BM Market Price (€/MWh)'].interpolate(method='pad')\ncombined['Market Net Imbalance Volume (MW)'] = combined['Market Net Imbalance Volume (MW)'].interpolate(method='pad')\ncombined['Actual Wind (MW)'] = combined['Actual Wind (MW)'].interpolate(method='pad')\n\n# # IDA2 and IDA2 only the 8 rows should be interpolated (we dealt with this by converting all missing values into 0 earlier so that only new rows remain as NaN)\ncombined['IDA2 Market Price (€/MWh)'] = combined['IDA2 Market Price (€/MWh)'].interpolate()\ncombined['IDA3 Market Price (€/MWh)'] = combined['IDA3 Market Price (€/MWh)'].interpolate()\n# NOTE: 8 missing rows at 1 and 1:30 => IDA2 and IDA3 markets are closed. Might as well replace missing values in these columns with 0",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# check for NaNs\ncombined.isna().sum()",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Build new column Net Demand",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "To help with training, we created a new column `net demand` that holds the variables created by the given variable, which represents the difference between electricity demand and wind power generation.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Build net demand column\ncombined[\"Net Demand\"] = combined[\"Demand (MW)\"] - combined[\"Actual Wind (MW)\"]\ncombined.head()\ncombined.shape",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Data Exploration \nIn the data exploration, we worked out how each variable changes along the time, which is the univariate analysis; how each variable is related with others or its past data, which is the multivariate analysis; and outlier detection. We visualized time series data to achieve the above objectives. These visualization graphs can provide informative insights to specify time-related features, such as trends, cycles, and seasonality, that may affect the model selection (Brownlee, 2019).",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Univariate analysis",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "##### Wind & Demand Plotted By Date Across the Entire Dataset",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Here we first plotted the demand, wind and net demand in a line plot, shows how the values changed with the time. From the plots, we can clearly see the existence of cycles, roughly one year at a time, with demand and wind being more cyclical and net demand being essentially flat, because it is the difference between the two above, but there is still some small downward arc, because the gap is larger near the beginning and end of the cycle.  ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create copy as the index is being changed\ndemand_wind_graph= combined.copy()\n\ndemand_wind_graph = demand_wind_graph.drop(columns=['Trade Date', 'Start Time 30 Minute Period', 'DAM Market Price (€/MWh)', 'IDA1 Market Price (€/MWh)', 'IDA2 Market Price (€/MWh)', 'IDA3 Market Price (€/MWh)',\n                                                    'BM Market Price (€/MWh)', 'Market Net Imbalance Volume (MW)', 'Gas Price £/Therm', 'GB Price (€/MWh)'])\n    \n# Set date as index so it's x axis by default\ndemand_wind_graph.set_index('Start Date', inplace = True)\n\n# axes = demand_wind_graph.plot(sharex='all', subplots=True, title=['Forecast Wind Production', 'Forecast Demand', \"Net Demand\"], figsize=(20,7.5))\naxes = demand_wind_graph.plot(sharex='all', subplots=True,  figsize=(20,7.5))\n\naxes[0].xaxis.set_tick_params(labelbottom=True)\naxes[0].set_ylabel(\"Demand MWh\")\n\naxes[1].xaxis.set_tick_params(labelbottom=True)\naxes[1].set_ylabel(\"Wind Production MWh\")\n\naxes[2].set_ylabel(\"Net Demand (Demand - Wind)\")\naxes[2].set_xlabel(\"Date Period\")\n\n#Adjust plots to create gaps for legibility\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1,\n                    right=0.9,\n                    top=0.9,\n                    wspace=0.6,\n                    hspace=0.6)\nplt.show();",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Summary Statistics for Forecast Wind Production & Demand By Month",
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": "# Splits demand and wind by year based on index\n#Runs from 2300-2300\n\nd_w_2021= demand_wind.iloc[0:17520]\nd_w_2022= demand_wind.iloc[17520:35039]\nd_w_2023= demand_wind.iloc[35039:]",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T15:33:50.670214Z",
          "iopub.status.busy": "2023-05-03T15:33:50.668575Z",
          "iopub.status.idle": "2023-05-03T15:33:50.679519Z",
          "shell.execute_reply": "2023-05-03T15:33:50.678768Z",
          "shell.execute_reply.started": "2023-05-03T15:33:50.670178Z"
        }
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "d_w_2021_sum_stats = d_w_2021.copy()\nd_w_2021_sum_stats['month'] = pd.DatetimeIndex(d_w_2021_sum_stats['Start Date'], dayfirst=True).month\nd_w_2021_sum_stats.groupby(\"month\").describe()",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Gas & Electricity Price Plotted",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Next we plot the gas and electricity price.Even if these two graphs do not have exactly the same shape, we can still observe that prices show great volatility between October 2021 and April 2022. There is another peak between July to October of 2022. The price increased again and reached peak at around the end of the 2022. The trend and the position that has the peak values are nearly the same. This suggests that gas and electricity prices always have similar movements, which could be due to some reason or they are affected in the same way.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create a merged DF\ngas_vs_elec = pd.merge(demand_electricity, demand_gas_half_hour, on=[\"Start Date\", \"Trade Date\", \"Start Time 30 Minute Period\"])\n",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T15:33:50.870121Z",
          "iopub.status.busy": "2023-05-03T15:33:50.869835Z",
          "iopub.status.idle": "2023-05-03T15:33:50.900903Z",
          "shell.execute_reply": "2023-05-03T15:33:50.899889Z",
          "shell.execute_reply.started": "2023-05-03T15:33:50.870096Z"
        }
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "gas_vs_elec_graph = gas_vs_elec.copy()\n\ngas_vs_elec_graph.set_index('Start Date', inplace = True)\n#plot\naxes = gas_vs_elec_graph.plot(sharex='all', subplots=True, title=['GB Price (€/MWh) from 2020-2023', 'Gas Price £/Therm from 2020-2023'], figsize=(20,7.5))\n\n\naxes[0].xaxis.set_tick_params(labelbottom=True)\naxes[0].set_ylabel(\"Cost in € per MWh\")\n\naxes[1].xaxis.set_tick_params(labelbottom=True)\naxes[1].set_ylabel(\"Cost in £ per Therm\")\naxes[1].set_xlabel(\"Date Period\")\n\n#Adjust plots to create gaps for legibility\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1,\n                    right=0.9,\n                    top=0.9,\n                    wspace=0.6,\n                    hspace=0.6)\nplt.show();",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Summary Statistics for Gas & Electricity by Year ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "gas_vs_elec_stats = gas_vs_elec.copy()\n\ngas_vs_elec_stats['year'] = pd.DatetimeIndex(gas_vs_elec_stats['Start Date'], dayfirst=True).year\ngas_vs_elec_stats.groupby(\"year\").describe()",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<!-- ##### Plotting Markets Across the Dataset -->\n\nThen we ploted the markets prices. The plots include 5 market and the Market Net Imbalance Volume (NIV).The plot shows the fluctuation of energy prices and NIV over time, which can be useful for understanding the trends and making informed decisions regarding energy consumption and cost management. The plot also provides a visual representation of the relationship between the different energy prices and the Market Net Imbalance Volume over the selected time period.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#Plotting each market across the data set\n\nprices_and_niv_graph = demand_price_niv.copy()\n\nprices_and_niv_graph.set_index('Start Date', inplace = True)\n\n#plot\naxes = prices_and_niv_graph.plot(sharex='all', subplots=True, title=['DAM Market Price (€/MWh)',\n                                                                     'IDA1 Market Price (€/MWh)',\n                                                                     \"IDA2 Market Price (€/MWh)\",\n                                                                     \"IDA3 Market Price (€/MWh)\",\n                                                                     \"BM Market Price (€/MWh)\",\n                                                                     \"Market Net Imbalance Volume (MW)\"], \n                                 figsize=(20,7.5))\n\n\n#Adjust plots to create gaps for legibility\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1,\n                    right=0.9,\n                    top=0.9,\n                    wspace=0.6,\n                    hspace=0.6)\nplt.show();",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "If we increase the size of each plot to obtain a bigger and more detailed plot, we can observe that nearly all five markets share the same trend, they increase and decrease with similar slope and reach peak around the same time. This means the price of these five markets are actually closely correlated with each other, and we will talk about this later in the multivariate analysis secton. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Outlier Detection\n\nOutliers are data points that lie far away from the majority and dramatically deviates from other observations in a dataset(2023). They are usually considerably larger or smaller than other values (Bonthu, 2023). The existence of outliers may have an impact on the result and  could result from things like measurement errors or odd occurrences.  \n\n<!-- ##### Demand -->\n\nThe graph shows the trend of the electricity demand over time, with each data point representing the demand for electricity at a specific point in time.The graph can help to identify patterns in electricity demand, such as seasonal fluctuations or changes in demand due to specific events as well as detect for outliers. Since we know the outliers are usually larger or smaller than other values, and from this graph we observed that the outliers are all in the lower part of the graph, so we try to detect outliers by finding values that is the smallest of all numbers. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# plot Demand over time\ncombined.plot('Start Date', 'Demand (MW)', kind='line', figsize=(15,3));",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "By using the 'nsmallest()' method to display the \"Demand (MW)\" column's 10 smallest values, the code may spot outliers. These 10 examples had the lowest demand in comparison to the rest of the dataset. These 10 data points reflect an extremely low demand compared to the rest of the dataset, and outliers are data points that drastically depart from the rest of the data.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Get 10 smallest values for demand\ncombined.nsmallest(10, 'Demand (MW)')",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<!-- ##### Gas -->\n\nWe plot gas to check for the outliers as well. We can see some values that are much higher than the rest of the data, indicating a potential anomaly or unusual event, hence these values are considered outliers. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# plot Gas price over time\ncombined.plot('Start Date', 'Gas Price £/Therm', kind='line', figsize=(15,3));",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Similar to above, we use the `nlargest()` function to get the 10 maximum values of the gas price column and treat them as anomalies.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Get 10 largest values for gas price\ncombined.nlargest(10, 'Gas Price £/Therm')",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "<!-- ##### Electricity -->\n\nAs what we did before, we plot the electricity prices and we can observe that there are some abnormal values in the first season of 2021, from October 2021 to February 2022, and around December 2022. All these values are significantly larger than others. So we keep using `nlargest()` function to get the five largest values as anomalies. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# plot GB price over time\ncombined.plot('Start Date', 'GB Price (€/MWh)', kind='line', figsize=(15,3));\n",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Get 5 largest values for GB price/MWh\ncombined.nlargest(5, 'GB Price (€/MWh)')",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# comparing prices with that of 2021\ncombined[combined['Start Date'] == \"2021-12-12\"].head(3)",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Multivariate Analysis",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "##### Correlation Matrix For Variables of Interest",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import seaborn as sns\nsns.heatmap(combined.corr(),annot=True, cbar=False, cmap='Blues', fmt='.1f');",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Autocorrelation",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We first work on the autocorrelation of each variable with `pd.autocorr()` function, this will look into the relation of past data and future data. The parameter `lag` means the period of time that compare with itself, more specifically is the number of month.  We selected 4 different numbers represents the autocorrelation between every month, season, half year and year.  \n- lag = 1: calculate the correlation between month M and month M-1. \n- lag = 3: calculate the correlation between month M and month M-3, which is every season\n- lag = 6: calculate the correlation between month M and month M-6, which is every half year\n- lag = 12: calculate the correlation between month M and month M-12, which is every year",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "##### Wind",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Auto-correlation for wind\nwind_autocorrelation_lag1 = combined['Actual Wind (MW)'].autocorr(lag=1)\nwind_autocorrelation_lag3 = combined['Actual Wind (MW)'].autocorr(lag=3)\nwind_autocorrelation_lag6 = combined['Actual Wind (MW)'].autocorr(lag=6)\n# wind_autocorrelation_lag9 = combined['Actual Wind (MW)'].autocorr(lag=9)\nwind_autocorrelation_lag12 = combined['Actual Wind (MW)'].autocorr(lag=12)\n\nprint(\" 1 Month Lag for wind: \", wind_autocorrelation_lag1)\nprint(\" 3 Month Lag for wind: \", wind_autocorrelation_lag3)\nprint(\" 6 Month Lag for wind: \", wind_autocorrelation_lag6)\n# print(\" 9 Month Lag for wind: \", wind_autocorrelation_lag9)\nprint(\"12 Month Lag for wind: \", wind_autocorrelation_lag12)\n",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Demand",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Auto-correlation for demand\ndemand_autocorrelation_lag1 = combined['Demand (MW)'].autocorr(lag=1)\ndemand_autocorrelation_lag3 = combined['Demand (MW)'].autocorr(lag=3)\ndemand_autocorrelation_lag6 = combined['Demand (MW)'].autocorr(lag=6)\ndemand_autocorrelation_lag12 = combined['Demand (MW)'].autocorr(lag=12)\n\nprint(\" 1 Month Lag for demand: \", demand_autocorrelation_lag1)\nprint(\" 3 Month Lag for demand: \", demand_autocorrelation_lag3)\nprint(\" 6 Month Lag for demand: \", demand_autocorrelation_lag6)\nprint(\"12 Month Lag for demand: \", demand_autocorrelation_lag12)\n",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Net Demand",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Auto-correlation for net demand\nnet_demand_autocorrelation_lag1 = combined['Net Demand'].autocorr(lag=1)\nnet_demand_autocorrelation_lag3 = combined['Net Demand'].autocorr(lag=3)\nnet_demand_autocorrelation_lag6 = combined['Net Demand'].autocorr(lag=6)\nnet_demand_autocorrelation_lag12 = combined['Net Demand'].autocorr(lag=12)\n\nprint(\" 1 Month Lag for net demand: \", net_demand_autocorrelation_lag1)\nprint(\" 3 Month Lag for net demand: \", net_demand_autocorrelation_lag3)\nprint(\" 6 Month Lag for net demand: \", net_demand_autocorrelation_lag6)\nprint(\"12 Month Lag for net demand: \", net_demand_autocorrelation_lag12)",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Gas Price",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Auto-correlation for gas price\ngas_price_autocorrelation_lag1 = combined['Gas Price £/Therm'].autocorr(lag=1)\ngas_price_autocorrelation_lag3 = combined['Gas Price £/Therm'].autocorr(lag=3)\ngas_price_autocorrelation_lag6 = combined['Gas Price £/Therm'].autocorr(lag=6)\ngas_price_autocorrelation_lag12 = combined['Gas Price £/Therm'].autocorr(lag=12)\n\nprint(\" 1 Month Lag for gas price: \", gas_price_autocorrelation_lag1)\nprint(\" 3 Month Lag for gas price: \", gas_price_autocorrelation_lag3)\nprint(\" 6 Month Lag for gas price: \", gas_price_autocorrelation_lag6)\nprint(\"12 Month Lag for gas price: \", gas_price_autocorrelation_lag12)",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### GB Electricity Prices",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Auto-correlation for electricity price\nelec_price_autocorrelation_lag1 = combined['GB Price (€/MWh)'].autocorr(lag=1)\nelec_price_autocorrelation_lag3 = combined['GB Price (€/MWh)'].autocorr(lag=3)\nelec_price_autocorrelation_lag6 = combined['GB Price (€/MWh)'].autocorr(lag=6)\nelec_price_autocorrelation_lag12 = combined['GB Price (€/MWh)'].autocorr(lag=12)\n\nprint(\" 1 Month Lag for electricity price: \", elec_price_autocorrelation_lag1)\nprint(\" 3 Month Lag for electricity price: \", elec_price_autocorrelation_lag3)\nprint(\" 6 Month Lag for electricity price: \", elec_price_autocorrelation_lag6)\nprint(\"12 Month Lag for electricity price: \", elec_price_autocorrelation_lag12)",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Prices - DAM",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Auto-correlation for DAM market\ndam_autocorrelation_lag1 = combined['DAM Market Price (€/MWh)'].astype('float32').autocorr(lag=1)\ndam_autocorrelation_lag3 = combined['DAM Market Price (€/MWh)'].astype('float32').autocorr(lag=3)\ndam_autocorrelation_lag6 = combined['DAM Market Price (€/MWh)'].astype('float32').autocorr(lag=6)\ndam_autocorrelation_lag12 = combined['DAM Market Price (€/MWh)'].astype('float32').autocorr(lag=12)\n\nprint(\" 1 Month Lag for DAM market price: \", dam_autocorrelation_lag1)\nprint(\" 3 Month Lag for DAM market price: \", dam_autocorrelation_lag3)\nprint(\" 6 Month Lag for DAM market price: \", dam_autocorrelation_lag6)\nprint(\"12 Month Lag for DAM market price: \", dam_autocorrelation_lag12)",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Prices - IDA1",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Auto-correlation for IDA1 market\nida1_autocorrelation_lag1 = combined['IDA1 Market Price (€/MWh)'].astype('float32').autocorr(lag=1)\nida1_autocorrelation_lag3 = combined['IDA1 Market Price (€/MWh)'].astype('float32').autocorr(lag=3)\nida1_autocorrelation_lag6 = combined['IDA1 Market Price (€/MWh)'].astype('float32').autocorr(lag=6)\nida1_autocorrelation_lag12 = combined['IDA1 Market Price (€/MWh)'].astype('float32').autocorr(lag=12)\n\nprint(\" 1 Month Lag for IDA1 market price: \", ida1_autocorrelation_lag1)\nprint(\" 3 Month Lag for IDA1 market price: \", ida1_autocorrelation_lag3)\nprint(\" 6 Month Lag for IDA1 market price: \", ida1_autocorrelation_lag6)\nprint(\"12 Month Lag for IDA1 market price: \", ida1_autocorrelation_lag12)",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Prices - IDA2",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Auto-correlation for IDA2 market\nida2_autocorrelation_lag1 = combined['IDA2 Market Price (€/MWh)'].astype('float32').autocorr(lag=1)\nida2_autocorrelation_lag3 = combined['IDA2 Market Price (€/MWh)'].astype('float32').autocorr(lag=3)\nida2_autocorrelation_lag6 = combined['IDA2 Market Price (€/MWh)'].astype('float32').autocorr(lag=6)\nida2_autocorrelation_lag12 = combined['IDA2 Market Price (€/MWh)'].astype('float32').autocorr(lag=12)\n\nprint(\" 1 Month Lag for IDA2 market price: \", ida2_autocorrelation_lag1)\nprint(\" 3 Month Lag for IDA2 market price: \", ida2_autocorrelation_lag3)\nprint(\" 6 Month Lag for IDA2 market price: \", ida2_autocorrelation_lag6)\nprint(\"12 Month Lag for IDA2 market price: \", ida2_autocorrelation_lag12)",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Prices - IDA3",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Auto-correlation for IDA3 market\nida3_autocorrelation_lag1 = combined['IDA3 Market Price (€/MWh)'].astype('float32').autocorr(lag=1)\nida3_autocorrelation_lag3 = combined['IDA3 Market Price (€/MWh)'].astype('float32').autocorr(lag=3)\nida3_autocorrelation_lag6 = combined['IDA3 Market Price (€/MWh)'].astype('float32').autocorr(lag=6)\nida3_autocorrelation_lag12 = combined['IDA3 Market Price (€/MWh)'].astype('float32').autocorr(lag=12)\n\nprint(\" 1 Month Lag for IDA3 market price: \", ida3_autocorrelation_lag1)\nprint(\" 3 Month Lag for IDA3 market price: \", ida3_autocorrelation_lag3)\nprint(\" 6 Month Lag for IDA3 market price: \", ida3_autocorrelation_lag6)\nprint(\"12 Month Lag for IDA3 market price: \", ida3_autocorrelation_lag12)",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Prices - BM",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Auto-correlation for BM market\nbm_autocorrelation_lag1 = combined['BM Market Price (€/MWh)'].astype('float32').autocorr(lag=1)\nbm_autocorrelation_lag3 = combined['BM Market Price (€/MWh)'].astype('float32').autocorr(lag=3)\nbm_autocorrelation_lag6 = combined['BM Market Price (€/MWh)'].astype('float32').autocorr(lag=6)\nbm_autocorrelation_lag12 = combined['BM Market Price (€/MWh)'].astype('float32').autocorr(lag=12)\n\nprint(\" 1 Month Lag for BM market price: \", bm_autocorrelation_lag1)\nprint(\" 3 Month Lag for BM market price: \", bm_autocorrelation_lag3)\nprint(\" 6 Month Lag for BM market price: \", bm_autocorrelation_lag6)\nprint(\"12 Month Lag for BM market price: \", bm_autocorrelation_lag12)",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Stationarity",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "For stationality, we use Dickey Fuller test to help. This test will apply statistic hypothesis on the value, if the alternative hypothesis is rejected, then the data is stationary.  (Pierre, 2022)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "##### GB Electricity Prices",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#Electricity\n\n#Create copy to not affect original dataset\nelec_price_stationary = demand_electricity.copy()\n\n#Need to reset this to enable conversion \nelec_price_stationary[\"Start Date\"] = elec_price_stationary[\"Start Date\"].astype(str)\n\n#Make date time column\nelec_price_stationary['datetime'] = pd.to_datetime(elec_price_stationary['Start Date'] + ' ' + elec_price_stationary['Start Time 30 Minute Period'])\n\n#Keep only the columns of interest\nelec_price_stationary= elec_price_stationary.drop([\"Start Time 30 Minute Period\", \"Start Date\", \"Trade Date\"], axis = 1)\n\n#set datetime as index\nelec_price_stationary.set_index(['datetime'], inplace=True)",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T15:33:57.890489Z",
          "iopub.status.busy": "2023-05-03T15:33:57.890133Z",
          "iopub.status.idle": "2023-05-03T15:33:57.945458Z",
          "shell.execute_reply": "2023-05-03T15:33:57.944640Z",
          "shell.execute_reply.started": "2023-05-03T15:33:57.890442Z"
        }
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# The more negative the ADF, the more likely we are to reject the null\n# Null Hypothesis: Suggests the time series has a unit root, meaning it is non-stationary; it has some time dependent structure.\n# Alternative Hypothesis: Suggests the time series is stationary. It does not have time-dependent structure.\n\n#p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n#p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.\n\n\nfrom statsmodels.tsa.stattools import adfuller\nprint(\"Observations of Dickey-fuller test\")\nresult = adfuller(elec_price_stationary['GB Price (€/MWh)'],autolag='AIC')\n\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Gas Price",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Gas\n\n#Create copy to not affect original dataset\ngas_price_stationary = demand_gas_half_hour.copy()\n\n#Need to reset this to enable conversion \ngas_price_stationary[\"Start Date\"] = gas_price_stationary[\"Start Date\"].astype(str)\n\n#Make date time column\ngas_price_stationary['datetime'] = pd.to_datetime(gas_price_stationary['Start Date'] + ' ' + gas_price_stationary['Start Time 30 Minute Period'])\n\n#Keep only the columns of interest\ngas_price_stationary= gas_price_stationary.drop([\"Start Time 30 Minute Period\", \"Start Date\", \"Trade Date\"], axis = 1)\n\n#set datetime as index\ngas_price_stationary.set_index(['datetime'], inplace=True)\n",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T15:34:05.012921Z",
          "iopub.status.busy": "2023-05-03T15:34:05.008603Z",
          "iopub.status.idle": "2023-05-03T15:34:05.130988Z",
          "shell.execute_reply": "2023-05-03T15:34:05.130171Z",
          "shell.execute_reply.started": "2023-05-03T15:34:05.012849Z"
        }
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# The more negative the ADF, the more likely we are to reject the null\n# Null Hypothesis: Suggests the time series has a unit root, meaning it is non-stationary; it has some time dependent structure.\n# Alternative Hypothesis: Suggests the time series is stationary. It does not have time-dependent structure.\n\n#p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n#p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.\n\nfrom statsmodels.tsa.stattools import adfuller\nprint(\"Observations of Dickey-fuller test\")\nresult = adfuller(gas_price_stationary['Gas Price £/Therm'],autolag='AIC')\n\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Wind",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Wind\n\n#Create copy to not affect original dataset\nwind_stationary = combined.copy()\n\nwind_stationary[\"Start Date\"] = wind_stationary[\"Start Date\"].astype(str)\n\n#Make date time column\nwind_stationary['datetime'] = pd.to_datetime(wind_stationary['Start Date'] + ' ' + wind_stationary['Start Time 30 Minute Period'])\n\n#Keep only the columns of interest\nwind_stationary= wind_stationary.drop([\"Start Time 30 Minute Period\", \"Start Date\", \"Trade Date\", 'Net Demand', 'Demand (MW)'], axis = 1)\n\n#set datetime as index\nwind_stationary.set_index(['datetime'], inplace=True)\n",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T15:34:13.425936Z",
          "iopub.status.busy": "2023-05-03T15:34:13.425163Z",
          "iopub.status.idle": "2023-05-03T15:34:13.567604Z",
          "shell.execute_reply": "2023-05-03T15:34:13.566519Z",
          "shell.execute_reply.started": "2023-05-03T15:34:13.425896Z"
        }
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# The more negative the ADF, the more likely we are to reject the null\n# Null Hypothesis: Suggests the time series has a unit root, meaning it is non-stationary; it has some time dependent structure.\n# Alternative Hypothesis: Suggests the time series is stationary. It does not have time-dependent structure.\n\n#p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n#p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.\n\n\nfrom statsmodels.tsa.stattools import adfuller\nprint(\"Observations of Dickey-fuller test\")\nresult = adfuller(wind_stationary['Actual Wind (MW)'],autolag='AIC')\n\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Demand",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Demand\n\n#Create copy to not affect original dataset\ndemand_stationary = combined.copy()\n\ndemand_stationary[\"Start Date\"] = demand_stationary[\"Start Date\"].astype(str)\n\n#Make date time column\ndemand_stationary['datetime'] = pd.to_datetime(demand_stationary['Start Date'] + ' ' + demand_stationary['Start Time 30 Minute Period'])\n\n#Keep only the columns of interest\ndemand_stationary = demand_stationary.drop([\"Start Time 30 Minute Period\", \"Start Date\", \"Trade Date\", 'Net Demand', 'Actual Wind (MW)'], axis = 1)\n\n#set datetime as index\ndemand_stationary.set_index(['datetime'], inplace=True)\n",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T15:34:21.381439Z",
          "iopub.status.busy": "2023-05-03T15:34:21.380674Z",
          "iopub.status.idle": "2023-05-03T15:34:21.503254Z",
          "shell.execute_reply": "2023-05-03T15:34:21.502440Z",
          "shell.execute_reply.started": "2023-05-03T15:34:21.381383Z"
        }
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# The more negative the ADF, the more likely we are to reject the null\n# Null Hypothesis: Suggests the time series has a unit root, meaning it is non-stationary; it has some time dependent structure.\n# Alternative Hypothesis: Suggests the time series is stationary. It does not have time-dependent structure.\n\n#p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n#p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.\n\n\nfrom statsmodels.tsa.stattools import adfuller\nprint(\"Observations of Dickey-fuller test\")\nresult = adfuller(demand_stationary['Demand (MW)'],autolag='AIC')\n\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Decomposition",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "By spliting a time series data into several components, each representing an underlying pattern category.We can see the decomposition plot comprising three components: a trend-cycle component, a seasonal component, and a remainder component (containing anything else in the time series)().",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "##### Wind",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Decomposition for wind\n# The \"period\" parameter is the number of observations in a seasonal cycle. \n# For example, if you have daily observations and weekly seasonality, the period is 7. \n# Hence period here is 365 days a year * 48 time slots a day = 17520 \n \nwind_decompose = seasonal_decompose(combined['Actual Wind (MW)'],model='additive', period=17520)\nwind_decompose.plot()\nplt.show()",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Demand",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "demand_decompose = seasonal_decompose(combined['Demand (MW)'],model='additive', period=17520)\ndemand_decompose.plot()\nplt.show()",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Net Demand",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "net_demand_decompose = seasonal_decompose(combined['Net Demand'],model='additive', period=17520)\nnet_demand_decompose.plot()\nplt.show()",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Gas Price",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "gas_decompose = seasonal_decompose(combined['Gas Price £/Therm'],model='additive', period=17520)\ngas_decompose.plot()\nplt.show()",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "##### Electricity Prices",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "gb_price_decompose = seasonal_decompose(combined['GB Price (€/MWh)'],model='additive', period=17520)\ngb_price_decompose.plot()\nplt.show()",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Modelling",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "To handle time sequence data, LSTM is a good choice. LSTM models are a type of recurrent neural network that are designed to capture these temporal dependencies in time series data. The unique architecture of LSTM models allows them to selectively remember and forget information from previous time steps, making them well-suited for processing long sequences of data.  It can effectively capture dependencies and patterns that exist over long time lags in the data. \n\nThe input data was split into train, validation and test. The test data contains the last 48 time slots of a day to evaluate the predictions. We splited the rest of the data into 80/20, which train data accounts for 80% and validation accounts for 20%. \n\nInitially, we decided to use a single model to predict prices of all markets together, however due to the different opening times of IDA2 and IDA3 market, the model results in extremely high loss during training and bad predictions. Therefore, we determined to build 3 models - one for DAM, IDA1, BM since they are all open 24 hours, one for IDA2 and another for IDA3. For the closed period of IDA2 and IDA3, we decided to fill the NAN with the median values of that day after several attempts. \n\nThe training loss of each model is shown as follows. These curves decrease as the training process proceeds and then reaches a stable point. After this, we apply the model to the test dataset and creates a list of prediction. By calculating the Root Mean Squared Error(RMSE) between the prediction and our true values, we will know the performance of our model on test set, and the smaller the RMSE is, the closer the prediction is to the real data, the better result we have. The average RMSE of these three tests are within the range 40-50, which means the model works but still could be improved. This is thelimitation of out model and we will discuss this in the later part.\n\nAfter finishing testing the data of the last day, we append our predictions to the “combined” dataframe which saves all our variables, and the columns name add “prediction” at the start. So now we have both predictions and true values in the same data frame and we can compare them easily. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# import libraries for models\n\nfrom numpy import array\nfrom numpy import hstack\nfrom numpy import array\nfrom numpy import hstack\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T15:34:37.339031Z",
          "iopub.status.busy": "2023-05-03T15:34:37.338645Z",
          "iopub.status.idle": "2023-05-03T15:34:44.566923Z",
          "shell.execute_reply": "2023-05-03T15:34:44.565768Z",
          "shell.execute_reply.started": "2023-05-03T15:34:37.338995Z"
        }
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Interpolate 0 values in IDA2 and IDA3 before model training\n\ncombined_bkp = combined.copy()\ndf= combined.copy()\n\n#Group by Start Date\ndf_grouped = df.groupby(df['Start Date'].dt.date)\n\n# Calculate the mean of non-zero values for date in ID2\nmedian_ida2 = df_grouped['IDA2 Market Price (€/MWh)'].transform(lambda x: x[x != 0].median())\nmedian_ida3 = df_grouped['IDA3 Market Price (€/MWh)'].transform(lambda x: x[x != 0].median())\n\n#In case of error; unlikely a few 0s will affcect prediction that badly\nmedian_ida2.fillna(0, inplace = True)\nmedian_ida3.fillna(0, inplace = True)\n\n# Fill the zeros with the mean value for each day\ncombined['IDA2 Market Price (€/MWh)'] = df[\"IDA2 Market Price (€/MWh)\"].mask(df['IDA2 Market Price (€/MWh)'] == 0, median_ida2)\ncombined['IDA3 Market Price (€/MWh)'] = df[\"IDA3 Market Price (€/MWh)\"].mask(df['IDA3 Market Price (€/MWh)'] == 0, median_ida3)",
      "metadata": {},
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Define input sequences\n\n# get column values for reshaping\nwind_input = combined[\"Actual Wind (MW)\"]\ndemand_input = combined[\"Demand (MW)\"]\nnet_demand_input = combined[\"Net Demand\"]\ngas_input = combined[\"Gas Price £/Therm\"]\nelec_input = combined[\"GB Price (€/MWh)\"]\ndm_output = combined[\"DAM Market Price (€/MWh)\"]\nida1_output = combined[\"IDA1 Market Price (€/MWh)\"]\nida2_output = combined[\"IDA2 Market Price (€/MWh)\"]\nida3_output = combined[\"IDA3 Market Price (€/MWh)\"]\nbm_output = combined['BM Market Price (€/MWh)']\n\n# reshape\nwind_input = wind_input.values.reshape((-1, 1))\ndemand_input = demand_input.values.reshape((-1, 1))\nnet_demand_input = net_demand_input.values.reshape((-1, 1))\ngas_input = gas_input.values.reshape((-1, 1))\nelec_input = elec_input.values.reshape((-1, 1))\ndm_output = dm_output.values.reshape((-1,1))\nida1_output = ida1_output.values.reshape((-1,1))\nida2_output = ida2_output.values.reshape((-1,1))\nida3_output = ida3_output.values.reshape((-1,1))\nbm_output = bm_output.values.reshape((-1,1))\n\n\n# horizontally stack columns\n\n# for markets DAM, ID1 and BM\ndataset = hstack((wind_input, demand_input, net_demand_input, gas_input, elec_input, dm_output, ida1_output, bm_output))\n# for IDA2 market\ndataset_ida2 = hstack((wind_input, demand_input, net_demand_input, gas_input, elec_input, ida2_output))\n# for IDA3 market\ndataset_ida3 = hstack((wind_input, demand_input, net_demand_input, gas_input, elec_input, ida3_output))\n\n# create train, validation and test datasets for all three cases\ntest_data_3M = dataset[-53:] # add extra rows to get 48 rows at the end as this is also done in batches\ntrain_data_3M = dataset[0:31488]\nval_data_3M = dataset[31488:-53]\n\ntest_data_ida2 = dataset_ida2[-53:] ############################\ntrain_data_ida2 = dataset_ida2[0:31488]\nval_data_ida2 = dataset_ida2[31488:-53]\n\ntest_data_ida3 = dataset_ida3[-53:]\ntrain_data_ida3 = dataset_ida3[0:31488]\nval_data_ida3 = dataset_ida3[31488:-53]\n\n\n# print(dataset[0])\n# print(\"--------\")\n# print(dataset_ida2[0])\n# print(\"--------\")\n# print(dataset_ida3[0])",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T15:34:44.569054Z",
          "iopub.status.busy": "2023-05-03T15:34:44.568326Z",
          "iopub.status.idle": "2023-05-03T15:34:44.586874Z",
          "shell.execute_reply": "2023-05-03T15:34:44.585993Z",
          "shell.execute_reply.started": "2023-05-03T15:34:44.569021Z"
        }
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# ensure split is proper\nlen(val_data_3M) + len(train_data_3M) + len(test_data_3M)\nlen(val_data_ida2) + len(train_data_ida2) + len(test_data_ida2)\nlen(val_data_ida3) + len(train_data_ida3) + len(test_data_ida3)",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# test_data_3M[-1,:-3]\n# test_data_3M[-1,-3:]\n# # [1250.      , 4498.      , 3248.      ,  114.3     ,  118.1856  ,         121.4     ,  110.      ,  112.88    ,  112.88    ,  170.94    ]\n# print(\" \")\n# test_data_ida2[-1,:-1]\n# test_data_ida2[-1,-1]",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T15:34:44.603385Z",
          "iopub.status.busy": "2023-05-03T15:34:44.602961Z",
          "iopub.status.idle": "2023-05-03T15:34:44.608517Z",
          "shell.execute_reply": "2023-05-03T15:34:44.607439Z",
          "shell.execute_reply.started": "2023-05-03T15:34:44.603350Z"
        }
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "### define functions to split multivariate sequence into samples\n\n# For DAM, IDA1 and BM Markets\ndef split_sequences_3M(sequences, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n    # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the dataset\n        if end_ix > len(sequences):\n            break\n        # gather input and output parts of the pattern\n        # seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n        seq_x, seq_y = sequences[i:end_ix, :-3], sequences[end_ix-1, -3:]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n\n\n# For IDA2 and IDA3 markets\ndef split_sequences_1M(sequences, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n    # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the dataset\n        if end_ix > len(sequences):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n\n# If we have 8 rows(A-H) altogether and our step is 4, it means we will first make ABCD as input and E as output. Then we will choose BCDE as input and F as output.",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T15:34:44.612841Z",
          "iopub.status.busy": "2023-05-03T15:34:44.611465Z",
          "iopub.status.idle": "2023-05-03T15:34:44.620351Z",
          "shell.execute_reply": "2023-05-03T15:34:44.619502Z",
          "shell.execute_reply.started": "2023-05-03T15:34:44.612810Z"
        }
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# define number of steps\nn_steps = 4\n\n# convert into input/output\n# For DAM, IDA1 and BM markets\nx_train_3M, y_train_3M = split_sequences_3M(train_data_3M, n_steps)\n# For IDA2 market\nx_train_ida2, y_train_ida2 = split_sequences_1M(train_data_ida2, n_steps)\n# For IDA3 market\nx_train_ida3, y_train_ida3 = split_sequences_1M(train_data_ida3, n_steps)\n\n# print(x_train_3M.shape, y_train_3M.shape)\n# print(x_train_3M[0], y_train_3M[0])\n# print(\"--------\")\n# print(x_train_ida2.shape, y_train_ida2.shape)\n# print(x_train_ida2[0], y_train_ida2[0])\n# print(\"--------\")\n# print(x_train_ida3.shape, y_train_ida3.shape)\n# print(x_train_ida3[0], y_train_ida3[0])",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T15:34:44.621904Z",
          "iopub.status.busy": "2023-05-03T15:34:44.621640Z",
          "iopub.status.idle": "2023-05-03T15:34:44.793711Z",
          "shell.execute_reply": "2023-05-03T15:34:44.792811Z",
          "shell.execute_reply.started": "2023-05-03T15:34:44.621881Z"
        }
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#--------------------------------------------------------------------------------------------------------------\n# Model for DAM, IDA1 and BM Markets\n#--------------------------------------------------------------------------------------------------------------\n\n# define model\nmodel_3M = Sequential()\nmodel_3M.add(LSTM(100,  input_shape=(n_steps, x_train_3M.shape[2])))\nmodel_3M.add(Dense(3))\n\n# compile model\nmodel_3M.compile(optimizer='adam', loss='mse')\n\n# fit model\nx_train_3M = np.asarray(x_train_3M).astype(np.float32)\ny_train_3M = np.asarray(y_train_3M).astype(np.float32)\n\n# Instantiate the model and save the loss\nhistory_3M = model_3M.fit(x_train_3M, y_train_3M, epochs=60, verbose=1)\n",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "trusted": true
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#--------------------------------------------------------------------------------------------------------------\n# Model for IDA2 market\n#--------------------------------------------------------------------------------------------------------------\n\n# define model\nmodel_ida2 = Sequential()\nmodel_ida2.add(LSTM(100,  input_shape=(n_steps, x_train_ida2.shape[2])))\nmodel_ida2.add(Dense(1))\n\n# Compile model\nmodel_ida2.compile(optimizer='adam', loss='mse')\n\n# fit model\nx_train_ida2 = np.asarray(x_train_ida2).astype(np.float32)\ny_train_ida2 = np.asarray(y_train_ida2).astype(np.float32)\n\n# Instantiate the model and save the loss\nhistory_ida2 = model_ida2.fit(x_train_ida2, y_train_ida2, epochs=60, verbose=1)\n",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "trusted": true
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#--------------------------------------------------------------------------------------------------------------\n# Model for IDA3 market\n#--------------------------------------------------------------------------------------------------------------\n\n# define model\nmodel_ida3 = Sequential()\nmodel_ida3.add(LSTM(100,  input_shape=(n_steps, x_train_ida3.shape[2])))\n# model_ida3.add(LSTM(100))\nmodel_ida3.add(Dense(1))\n\n# Compile model\nmodel_ida3.compile(optimizer='adam', loss='mse')\n\n# fit model\nx_train_ida3 = np.asarray(x_train_ida3).astype(np.float32)\ny_train_ida3 = np.asarray(y_train_ida3).astype(np.float32)\n\n# Instantiate the model and save the loss\nhistory_ida3 = model_ida3.fit(x_train_ida3, y_train_ida3, epochs=60, verbose=1)\n",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "trusted": true
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Save loss outputs to enable plotting\nhist_df_3M = pd.DataFrame(history_3M.history) \nhist_df_ida2 = pd.DataFrame(history_ida2.history) \nhist_df_ida3 = pd.DataFrame(history_ida3.history) \n\nplt.plot(history_3M.history['loss'], label='Training loss DAM, IDA1, BM')\nplt.plot(history_ida2.history['loss'], label='Training loss IDA2')\nplt.plot(history_ida3.history['loss'], label='Training loss IDA3')\nplt.legend();",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# create sequences for validation\n\nx_val_3M, y_val_3M = split_sequences_3M(val_data_3M, n_steps)\nx_val_ida2, y_val_ida2 = split_sequences_1M(val_data_ida2, n_steps)\nx_val_ida3, y_val_ida3 = split_sequences_1M(val_data_ida3, n_steps)\n\n# print(x_val_3M.shape, y_val_3M.shape)\n# print(x_val_ida2.shape, y_val_ida2.shape)\n# print(x_val_ida3.shape, y_val_ida3.shape)\n\n# print(x_val_3M[0], y_val_3M[0])\n# print(x_val_ida2[0], y_val_ida2[0])\n# print(x_val_ida3[0], y_val_ida3[0])\n",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T15:51:29.341139Z",
          "iopub.status.busy": "2023-05-03T15:51:29.340721Z",
          "iopub.status.idle": "2023-05-03T15:51:29.403266Z",
          "shell.execute_reply": "2023-05-03T15:51:29.402161Z",
          "shell.execute_reply.started": "2023-05-03T15:51:29.341101Z"
        }
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Predict for DAM, ID1, BM Markets\nx_val_3M = np.asarray(x_val_3M).astype(np.float32)\ny_val_3M = np.asarray(y_val_3M).astype(np.float32)\nprediction_3M = model_3M.predict(x_val_3M, verbose=1)\n# print(prediction_3M)",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Predict for IDA2 market\nx_val_ida2 = np.asarray(x_val_ida2).astype(np.float32)\ny_val_ida2 = np.asarray(y_val_ida2).astype(np.float32)\nprediction_ida2 = model_ida2.predict(x_val_ida2, verbose=1)\n# print(prediction_ida2)",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Predict for IDA3 market\nx_val_ida3 = np.asarray(x_val_ida3).astype(np.float32)\ny_val_ida3 = np.asarray(y_val_ida3).astype(np.float32)\nprediction_ida3 = model_ida3.predict(x_val_ida3, verbose=1)\n# print(prediction_ida3)",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Check root mean square error on validation data\n\nrmse_3M = []\nrmse_ida2 = []\nrmse_ida3 = []\n\nfor i in range(len(y_val_3M)):\n    rmse_3M.append(np.sqrt(np.mean((prediction_3M[i] - y_val_3M[i])**2)))\n    \nfor i in range(len(y_val_ida2)):\n    rmse_ida2.append(np.sqrt(np.mean((prediction_ida2[i] - y_val_ida2[i])**2)))\n\nfor i in range(len(y_val_ida3)):\n    rmse_ida3.append(np.sqrt(np.mean((prediction_ida3[i] - y_val_ida3[i])**2)))\n\n# Find minimum, average and maximum RMSE values\nprint(\"Statistics for DAM, IDA1 and BM markets\")\nprint(\"----------------------------------------\")\nprint(f\"max: {max(rmse_3M)}, min:{min(rmse_3M)}, avg: {np.mean(rmse_3M)}\")\n\nprint(\"IDA2 market\")\nprint(\"----------------------------------------\")\nprint(f\"max: {max(rmse_ida2)}, min:{min(rmse_ida2)}, avg: {np.mean(rmse_ida2)}\")\n\nprint(\"IDA3 market\")\nprint(\"----------------------------------------\")\nprint(f\"max: {max(rmse_ida3)}, min:{min(rmse_ida3)}, avg: {np.mean(rmse_ida3)}\")",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Prove RMSE decreases\n\n# Create subplots\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(12,4))\n\n# Plot the histograms on each subplot\nax1.hist(rmse_3M, bins=30, color='#45ADA8')\nax2.hist(rmse_ida2, bins=30, color='#9DE0AD')\nax3.hist(rmse_ida3, bins=30, color='#FFA500')\n\n# Set the axis labels and titles for each subplot\nax1.set_xlabel('RMSE')\nax1.set_ylabel('Probability')\nax1.set_title('Histogram of rmse_3M')\n\nax2.set_xlabel('RMSE')\nax2.set_ylabel('Probability')\nax2.set_title('Histogram of rmse_ida2')\n\nax3.set_xlabel('RMSE')\nax3.set_ylabel('Probability')\nax3.set_title('Histogram of rmse_ida3')\n\n# Show the plot\nplt.show();",
      "metadata": {
        "tags": [],
        "trusted": true
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# using test dataset to check\n\ntest_data_X_3M = test_data_3M[:, :-3] # all columns except last n\ntest_data_Y_3M = test_data_3M[:, -3:] # last n columns\n\ntest_data_X_ida2 = test_data_ida2[:, :-1] \ntest_data_Y_ida2 = test_data_ida2[:, -1:] \n\ntest_data_X_ida3 = test_data_ida3[:, :-1] \ntest_data_Y_ida3 = test_data_ida3[:, -1:] \n\n# print(\"--------\")\n# test_data_X_3M[3]\n# test_data_Y_3M[3]\n# print(\"--------\")\n# test_data_X_ida2[3]\n# test_data_Y_ida2[3]\n# print(\"--------\")\n# test_data_X_ida3[3]\n# test_data_Y_ida3[3]",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T16:07:59.428728Z",
          "iopub.status.busy": "2023-05-03T16:07:59.427783Z",
          "iopub.status.idle": "2023-05-03T16:07:59.434239Z",
          "shell.execute_reply": "2023-05-03T16:07:59.432938Z",
          "shell.execute_reply.started": "2023-05-03T16:07:59.428691Z"
        }
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Split into sequence\nn_steps = 4\n\ntest_data_X_3M, test_data_Y_3M = split_sequences_3M(test_data_3M, n_steps)\ntest_data_X_3M, test_data_Y_3M = split_sequences_3M(test_data_3M, n_steps)\n\ntest_data_X_ida2, test_data_Y_ida2 = split_sequences_1M(test_data_ida2, n_steps)\ntest_data_X_ida2, test_data_Y_ida2 = split_sequences_1M(test_data_ida2, n_steps)\n\ntest_data_X_ida3, test_data_Y_ida3 = split_sequences_1M(test_data_ida3, n_steps)\ntest_data_X_ida3, test_data_Y_ida3 = split_sequences_1M(test_data_ida3, n_steps)",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T16:08:08.040844Z",
          "iopub.status.busy": "2023-05-03T16:08:08.040491Z",
          "iopub.status.idle": "2023-05-03T16:08:08.047153Z",
          "shell.execute_reply": "2023-05-03T16:08:08.046243Z",
          "shell.execute_reply.started": "2023-05-03T16:08:08.040817Z"
        }
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Predict for DAM, IDA1 and BM markets on test data\ntest_data_X_3M = np.asarray(test_data_X_3M).astype(np.float32)\ntest_data_Y_3M = np.asarray(test_data_Y_3M).astype(np.float32)\nprediction_test_3M = model_3M.predict(test_data_X_3M, verbose=0)\n# print(prediction_test_3M) ",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T16:08:46.656947Z",
          "iopub.status.busy": "2023-05-03T16:08:46.656546Z",
          "iopub.status.idle": "2023-05-03T16:08:46.730196Z",
          "shell.execute_reply": "2023-05-03T16:08:46.729292Z",
          "shell.execute_reply.started": "2023-05-03T16:08:46.656913Z"
        },
        "scrolled": true
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Predict for IDA2 market on test data\ntest_data_X_ida2 = np.asarray(test_data_X_ida2).astype(np.float32)\ntest_data_Y_ida2 = np.asarray(test_data_Y_ida2).astype(np.float32)\nprediction_test_ida2 = model_ida2.predict(test_data_X_ida2, verbose=0)\n# print(prediction_test_ida2) ",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T16:09:06.088949Z",
          "iopub.status.busy": "2023-05-03T16:09:06.088554Z",
          "iopub.status.idle": "2023-05-03T16:09:06.157083Z",
          "shell.execute_reply": "2023-05-03T16:09:06.156218Z",
          "shell.execute_reply.started": "2023-05-03T16:09:06.088917Z"
        },
        "scrolled": true
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Predict for IDA3 market on test data\ntest_data_X_ida3 = np.asarray(test_data_X_ida3).astype(np.float32)\ntest_data_Y_ida3 = np.asarray(test_data_Y_ida3).astype(np.float32)\nprediction_test_ida3 = model_ida3.predict(test_data_X_ida3, verbose=0)\n# print(prediction_test_ida3)",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T16:09:17.761227Z",
          "iopub.status.busy": "2023-05-03T16:09:17.760805Z",
          "iopub.status.idle": "2023-05-03T16:09:17.833292Z",
          "shell.execute_reply": "2023-05-03T16:09:17.832238Z",
          "shell.execute_reply.started": "2023-05-03T16:09:17.761182Z"
        },
        "scrolled": true
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Results and Discussion\nWe apply the model on 31 March 2023’s data and measure its performance. From the plots that we can observed that the curve stays still until 11am, at which point it starts to increase and starts to follow the real trend. The model first selected IDA1 until 8am, and then BM until 11am, after that we choose IDA2. This result shows that the model under estimate the cost of IDA1 energy prices during this time, whilst overestimating DAM and BM. This illustrates a drawback of a basic LSTM model, which relies subsequent decisions in the time series on memory from prior data. Therefore, the best plan of action would be to only help purchasing selections made after 11 a.m. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Show test data with predictions\n\n# copy last 48 rows (last 1 day data)\ncombined_test = combined[-50:].copy() ###########################\n\n# create a combined dataframe with predictions for all markets\npredictions_DAM = pd.DataFrame(prediction_test_3M[:, 0], columns=['Predicted DAM Market Price'])\npredictions_IDA1 = pd.DataFrame(prediction_test_3M[:, 1], columns=['Predicted IDA1 Price'])\npredictions_IDA2 = pd.DataFrame(prediction_test_ida2[:, 0], columns=['Predicted IDA2 Price'])\npredictions_IDA3 = pd.DataFrame(prediction_test_ida3[:, 0], columns=['Predicted IDA3 Price'])\npredictions_BM = pd.DataFrame(prediction_test_3M[:, 2], columns=['Predicted BM Price'])\n\ncombined_test['Predicted DAM Market Price'] = predictions_DAM['Predicted DAM Market Price'].values\ncombined_test['Predicted IDA1 Price'] = predictions_IDA1['Predicted IDA1 Price'].values\ncombined_test['Predicted IDA2 Price'] = predictions_IDA2['Predicted IDA2 Price'].values\ncombined_test['Predicted IDA3 Price'] = predictions_IDA3['Predicted IDA3 Price'].values\ncombined_test['Predicted BM Price'] = predictions_BM['Predicted BM Price'].values\n\ncombined_test.drop(combined_test.tail(1).index,inplace=True) # pop off last row",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-05-03T16:10:11.978016Z",
          "iopub.status.busy": "2023-05-03T16:10:11.977391Z",
          "iopub.status.idle": "2023-05-03T16:10:12.048309Z",
          "shell.execute_reply": "2023-05-03T16:10:12.047381Z",
          "shell.execute_reply.started": "2023-05-03T16:10:11.977966Z"
        }
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# copy original market values into another dataframe\n\noriginal_values = combined_test[[\"Start Date\", \"Trade Date\", \"Start Time 30 Minute Period\",\n                                 \"DAM Market Price (€/MWh)\",\n                                 \"IDA1 Market Price (€/MWh)\",\n                                 \"IDA2 Market Price (€/MWh)\",\n                                 \"IDA3 Market Price (€/MWh)\",\n                                 \"BM Market Price (€/MWh)\",  \n                                 ]].copy()\n\noriginal_values.head(1)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Gather market price predictions \n\npredicted_values = combined_test.drop([\"Gas Price £/Therm\", \"GB Price (€/MWh)\",\n                    \"Demand (MW)\", \"Actual Wind (MW)\", \"Net Demand\",\n                    \"Trade Date\", \"Start Date\",\n                     \"Start Time 30 Minute Period\",\n                     \"DAM Market Price (€/MWh)\",\n                     \"IDA1 Market Price (€/MWh)\",\n                     \"IDA2 Market Price (€/MWh)\",\n                     \"IDA3 Market Price (€/MWh)\",\n                     \"BM Market Price (€/MWh)\",\n                     \"Market Net Imbalance Volume (MW)\"], axis = 1)\n\npredicted_values.head(1)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Create 30 minute intervals to append later\ntime_intervals = combined_test[[\"Start Time 30 Minute Period\"]].copy()",
      "metadata": {},
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Function to identify most profitable market\n\ndef profit_detector (df):\n    \n    \n    highest_col = []\n    # Create list to hold the highest profit value for each row\n    highest_vals = []\n    markets = ['DAM', 'IDA1', 'IDA2', 'IDA3', 'BM']\n    \n\n    for i, row in df.iterrows():\n        \n        # Create a list to hold the proft values calculated for each row after applying the equation\n        row_vals = []\n        index_loc= []\n\n\n        # Iterate through columns and calculate the value based on Energia's profitability formula\n        for j in range(0, 5): \n    \n            # as IDA2 And 3 have 0 values, we need to pass over these\n            if row[j] == 0:\n                pass\n            \n            elif row[j] > row[0]:\n                pass\n            \n            elif row[j] == row[0]:\n                # net profitability is zero when the values are same\n                row_vals.append(((row[0] - row[j]) * 100) / 2)\n                # also save corresponding index location \n                index_loc.append(j)\n                \n            elif row[j] < row[0]:\n                row_vals.append(((row[0] - row[j]) * 100) / 2)\n                # also save corresponding index location \n                index_loc.append(j)\n                \n            \n        # Find the index of the column that contains the highest profitability value contained in row_vals and save as var\n        max_col_index = row_vals.index(max(row_vals))\n\n        # Add the most profitable value and its corresponding column name to the respective list\n        highest_vals.append(max(row_vals))\n        highest_col.append(markets[index_loc[max_col_index]])\n        \n    # Rename the lists to make sense in the context of the data\n    df[\"Max predicted profitability (€/MWh)\"] = highest_vals\n    df[\"Most Profitable Predicted market\"] = highest_col\n    \n    # Return the data frame with the new columns\n    return df",
      "metadata": {},
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Apply a mask to the values that should be 0 in IDA2 & 3\n# They aren't 0 as they we're filled with the median \n\narge = pd.concat([time_intervals, predicted_values], axis=1)\narge['Predicted IDA2 Price'].mask((arge['Start Time 30 Minute Period'] <\"11:00:00\") | (arge['Start Time 30 Minute Period'] >\"17:00:00\"),'0', inplace=True)\narge['Predicted IDA3 Price'].mask((arge['Start Time 30 Minute Period'] <\"17:00:00\") | (arge['Start Time 30 Minute Period'] >\"23:00:00\"),'0', inplace=True)\narge.drop([\"Start Time 30 Minute Period\"], inplace = True, axis = 1)\narge = arge.astype(float)",
      "metadata": {},
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Get dataframe with profit details \ndf = profit_detector(arge)\nresult = pd.concat([original_values, df], axis=1)",
      "metadata": {},
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Show actual values and predictions side by side\n# result = (result[['Start Date', 'Trade Date', 'Start Time 30 Minute Period',\n#          'DAM Market Price (€/MWh)', 'Predicted DAM Market Price',\n#         'IDA1 Market Price (€/MWh)', 'Predicted IDA1 Price',\n#          'IDA2 Market Price (€/MWh)', 'Predicted IDA2 Price',\n#          'IDA3 Market Price (€/MWh)', 'Predicted IDA3 Price',\n#          'BM Market Price (€/MWh)', 'Predicted BM Price',\n#         \"Max predicted profitability (€/MWh)\", \"Profitable market\" \n#         ]]).copy()\n\n# predict for last day (48 intervals)\nresult",
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Make a DF of: DAMN, Actual DAM Minus Pred Market, Actual DAM Minus Actual Markets, Pred DAM Minus Pred Markets\n\n",
      "metadata": {
        "scrolled": true
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Gets DAMN and pred values\ntrue_dam = original_values[\"DAM Market Price (€/MWh)\"]\npred = predicted_values.drop(\"Predicted DAM Market Price\", axis = 1).copy()\ntesty = pred.assign(DAM = true_dam)\ntesty = testy[['DAM', 'Predicted IDA1 Price', 'Predicted IDA2 Price', 'Predicted IDA3 Price', \"Predicted BM Price\"]].copy()\ndam_on_pred = profit_detector (testy)\n",
      "metadata": {},
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Gets DAMN and actual\ntrue_vals = original_values.drop([\"Start Date\", \"Trade Date\", \"Start Time 30 Minute Period\"], axis =1).copy()\ndamn_on_true = profit_detector (true_vals)",
      "metadata": {},
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Make a DF of: DAMN, Actual DAM Minus Pred Market, Actual DAM Minus Actual Markets, Pred DAM Minus Pred Markets\n\n#Pred values\na = result[\"Max predicted profitability (€/MWh)\"].copy()\na = pd.DataFrame(a)\n\n#DAM\na['DAM'] = 0 \n\n#Start (x)\na[\"Start Time 30 Minute Period\"] = time_intervals # Pop in time intervals\n\n#Pred\na[\"Actual DAM Minus Pred\"] = dam_on_pred[\"Max predicted profitability (€/MWh)\"]\n\n#True\na[\"Actual Max Profit\"] = damn_on_true[\"Max predicted profitability (€/MWh)\"]",
      "metadata": {},
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "a.drop(\"Actual DAM Minus Pred\", axis = 1, inplace = True)",
      "metadata": {},
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#a=a.set_index('Start Time 30 Minute Period').copy()\n\nplt = a.plot.line()\n",
      "metadata": {
        "trusted": true
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "combined.tail(60)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Conclusion & Recommendation\n\nIn conclusion, the model has been successfully created to help predict market prices according to the given variables, however, for this project, it still doesn't adequately address Energia's business issues with the necessary level of realism. The shortage of time and complexity of the project make us not have adequate time on this project, therefore, to optimize the model and give better predictions, we give the following recommendations: \n1. Try to apply normalization to the data as pre-processing, since it will lead to significantly better results than the unnormalized data. (Hou et al., 2019)\n2. We only focus on building a model that could give predictions rather than a perfect model, hence we can apply optimization to the model to improve the performance. \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Reference\n1. Bonthu, H. (2023) Detecting and treating outliers: Treating the odd one out!, Analytics Vidhya. Available at: https://www.analyticsvidhya.com/blog/2021/05/detecting-and-treating-outliers-treating-the-odd-one-out/ (Accessed: May 7, 2023). \n2. Brownlee, J. (2019) Time series data visualization with python, MachineLearningMastery.com. Available at: https://machinelearningmastery.com/time-series-data-visualization-with-python/ (Accessed: May 7, 2023). \n3. Forecasting: Principles and practice (2nd ed) (no date) Chapter 6 Time series decomposition. Available at: https://otexts.com/fpp2/decomposition.html (Accessed: May 7, 2023). \n4. Hou, L., Zhu, J., T. Kwok, J., Gao, F., Qin, T. and Liu, T. (2019). Normalization Helps Training of Quantized LSTM. Advances in Neural Information Processing Systems 32 (NeurIPS 2019). [online] Available at: https://proceedings.neurips.cc/paper/2019/hash/f8eb278a8bce873ef365b45e939da38a-Abstract.html [Accessed 7 May 2023].\n5. Outlier (2023) Wikipedia. Wikimedia Foundation. Available at: https://en.wikipedia.org/wiki/Outlier (Accessed: May 7, 2023). \n6. Pierre, S. (2022) A guide to time series analysis in Python, Built In. Available at: https://builtin.com/data-science/time-series-python (Accessed: May 7, 2023).",
      "metadata": {}
    }
  ]
}